# ğŸš€ Gated Fusion Transformer ì¶”ê°€ ë°œì „ ë°©ì•ˆ

## í˜„ì¬ êµ¬í˜„ ìƒíƒœ (âœ… ì™„ë£Œ)
- âœ… Multi-Modal Query Fusion (video + text weighted sum)
- âœ… 3-way Gating (video + audio + text)
- âœ… Debug ì½”ë“œ ì¶”ê°€ (ì£¼ì„ ì²˜ë¦¬ëœ ìƒíƒœ)

---

## ğŸ“Š ìš°ì„ ìˆœìœ„ë³„ ê°œì„  ë°©ì•ˆ

### ğŸ¥‡ Priority 1: ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ 

#### 1ï¸âƒ£ **Layer-Aware Gating (ë ˆì´ì–´ë³„ ì ì‘ì  ê²Œì´íŒ…)** â­â­â­â­â­

**í•µì‹¬ ì•„ì´ë””ì–´**: ì´ˆë°˜ ë ˆì´ì–´ëŠ” ì˜¤ë””ì˜¤ ì •ë³´ë¥¼ ë§ì´ ë°›ê³ , í›„ë°˜ ë ˆì´ì–´ë¡œ ê°ˆìˆ˜ë¡ ì˜¤ë””ì˜¤ ì˜í–¥ì„ ì¤„ì„

**êµ¬í˜„ ë°©ë²• A: Simple Decay** (ê°€ì¥ ê°„ë‹¨)
```python
class ResidualAttentionBlock_Gate(nn.Module):
    def __init__(self, d_model: int, n_head: int, layer_idx: int, total_layers: int):
        super().__init__()
        # ... ê¸°ì¡´ ì½”ë“œ ...
        self.layer_ratio = layer_idx / total_layers  # 0.0 ~ 1.0
        
    def forward(self, para_tuple: tuple):
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        # Layer-aware decay
        layer_decay = 1.0 - self.layer_ratio  # 1.0 â†’ 0.0
        
        # Apply to audio gates
        attn_gate = attn_gate * layer_decay
        ff_gate = ff_gate * layer_decay
```

**êµ¬í˜„ ë°©ë²• B: Learnable Decay** (ë” ìœ ì—°í•¨)
```python
class Transformer_Gate(nn.Module):
    def __init__(self, width: int, layers: int, heads: int):
        super().__init__()
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        # Learnable layer decay parameter
        self.layer_decay = nn.Parameter(torch.ones(layers))
        
    def forward(self, q, v, t, attn_mask=None):
        attn_gate_list = []
        ff_gate_list = []
        query_gate_list = []
        
        x, v_out, t_out = q, v, t
        for i, block in enumerate(self.resblocks):
            x, v_out, t_out, _, attn_g, ff_g, query_g = block(
                (x, v_out, t_out, attn_mask, [], [], [])
            )
            
            # Apply learnable decay
            decay = torch.sigmoid(self.layer_decay[i])
            attn_g = attn_g * decay
            ff_g = ff_g * decay
            
            attn_gate_list.extend(attn_g)
            ff_gate_list.extend(ff_g)
            query_gate_list.extend(query_g)
```

**ê¸°ëŒ€ íš¨ê³¼**:
- ì´ˆë°˜: Low-level audio features (ì†Œë¦¬ì˜ ì§ˆê°, ë¦¬ë“¬ ë“±)
- ì¤‘ë°˜: Mid-level audio-visual alignment (ë™ì‘ê³¼ ì†Œë¦¬ì˜ ë™ê¸°í™”)
- í›„ë°˜: High-level semantic reasoning (ê°œë…ì  ì´í•´)

**ë‚œì´ë„**: â­â­â˜†â˜†â˜† (ì‰¬ì›€)  
**íš¨ê³¼**: â­â­â­â­â­ (ë§¤ìš° ë†’ìŒ)

---

#### 2ï¸âƒ£ **Attention Temperature Scaling** â­â­â­â­

**í•µì‹¬ ì•„ì´ë””ì–´**: Cross attentionì˜ "sharpness"ë¥¼ ì¡°ì ˆí•˜ì—¬ ë” focused/diffuseí•œ ìœµí•© ì œì–´

```python
class ResidualAttentionBlock_Gate(nn.Module):
    def __init__(self, d_model: int, n_head: int):
        super().__init__()
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        # Learnable temperature for cross attention
        self.temperature = nn.Parameter(torch.ones(1))
        
    def cross_attention(self, query, mem, attn_mask=None):
        if attn_mask is not None:
            attn_mask_ = attn_mask.repeat_interleave(self.n_head, dim=0)
            # Apply temperature scaling
            attn_mask_ = attn_mask_ / self.temperature
        else:
            attn_mask_ = attn_mask
        return self.cross_attn(query, mem, mem, need_weights=False, attn_mask=attn_mask_)[0]
```

**ê¸°ëŒ€ íš¨ê³¼**:
- Temperature â†‘: ë” ë¶€ë“œëŸ¬ìš´ ìœµí•© (ì—¬ëŸ¬ ì˜¤ë””ì˜¤ feature ê· ë“±í•˜ê²Œ)
- Temperature â†“: ë” ë‚ ì¹´ë¡œìš´ ìœµí•© (ê°€ì¥ ê´€ë ¨ìˆëŠ” ì˜¤ë””ì˜¤ë§Œ ì„ íƒ)

**ë‚œì´ë„**: â­â˜†â˜†â˜†â˜† (ë§¤ìš° ì‰¬ì›€)  
**íš¨ê³¼**: â­â­â­â˜†â˜† (ì¤‘ê°„)

---

#### 3ï¸âƒ£ **Residual Gate (Gated Residual Connection)** â­â­â­â­

**í•µì‹¬ ì•„ì´ë””ì–´**: ì˜¤ë””ì˜¤ ìœµí•©ì´ ì‹¤íŒ¨í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ skip connection ê°•í™”

```python
def forward(self, para_tuple: tuple):
    # ... ê¸°ì¡´ ì½”ë“œ ...
    
    # Compute audio fusion
    audio_contribution = self.cross_attention(self.ln_3(fused_query), v, attn_mask/100)
    
    # Residual gate: ì˜¤ë””ì˜¤ ê¸°ì—¬ë„ê°€ ë‚®ìœ¼ë©´ ìë™ìœ¼ë¡œ skip
    residual_weight = attn_gate.abs()  # Use magnitude as confidence
    x = x + audio_contribution * residual_weight
```

**ê¸°ëŒ€ íš¨ê³¼**:
- ë…¸ì´ì¦ˆê°€ ë§ì€ ì˜¤ë””ì˜¤ ì‹œí€€ìŠ¤ì—ì„œ ì•ˆì •ì 
- ì˜¤ë””ì˜¤ ì •ë³´ê°€ ë¶€ì¡±í•  ë•Œ ìë™ìœ¼ë¡œ video-only ëª¨ë“œë¡œ ì „í™˜

**ë‚œì´ë„**: â­â˜†â˜†â˜†â˜† (ë§¤ìš° ì‰¬ì›€)  
**íš¨ê³¼**: â­â­â­â­â˜† (ë†’ìŒ)

---

### ğŸ¥ˆ Priority 2: ì¤‘ê¸‰ ê°œì„  (ë” ë³µì¡í•˜ì§€ë§Œ íš¨ê³¼ì )

#### 4ï¸âƒ£ **Multi-Head Query Fusion** â­â­â­â­â­

**í•µì‹¬ ì•„ì´ë””ì–´**: ê° attention headë§ˆë‹¤ ë‹¤ë¥¸ video-text ìœµí•© ë¹„ìœ¨ ì‚¬ìš©

```python
class ResidualAttentionBlock_Gate(nn.Module):
    def __init__(self, d_model: int, n_head: int):
        super().__init__()
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        # Per-head query gate
        self.query_gate_per_head = nn.Sequential(OrderedDict([
            ("qg_fc", nn.Linear(int(d_model * 3), n_head, bias=False)),
        ]))
        
    def forward(self, para_tuple: tuple):
        # ... pooling ì½”ë“œ ...
        
        # Per-head gate weights: [batch, n_head]
        query_gate_weights = self.query_gate_per_head(
            torch.cat((x_mean, v_mean, t_mean), dim=1)
        ).sigmoid()
        
        # Split into heads and apply different weights
        # x: [seq_len, batch, d_model]
        batch_size = x.size(1)
        head_dim = self.d_model // self.n_head
        
        x_heads = x.view(x.size(0), batch_size, self.n_head, head_dim)
        t_heads = t.view(t.size(0), batch_size, self.n_head, head_dim)
        
        # Apply per-head fusion
        gate_broadcast = query_gate_weights.view(1, batch_size, self.n_head, 1)
        fused_heads = x_heads * gate_broadcast + t_heads * (1 - gate_broadcast)
        
        fused_query = fused_heads.view(x.size(0), batch_size, -1)
```

**ê¸°ëŒ€ íš¨ê³¼**:
- ì¼ë¶€ headëŠ” video-dominant (ì‹œê°ì  ì¥ë©´ ì´í•´)
- ì¼ë¶€ headëŠ” text-dominant (semantic grounding)
- ì¼ë¶€ headëŠ” balanced (multimodal reasoning)

**ë‚œì´ë„**: â­â­â­â˜†â˜† (ì¤‘ê°„)  
**íš¨ê³¼**: â­â­â­â­â­ (ë§¤ìš° ë†’ìŒ)

---

#### 5ï¸âƒ£ **Text-Conditioned Audio Transformation** â­â­â­â­

**í•µì‹¬ ì•„ì´ë””ì–´**: í…ìŠ¤íŠ¸ë¡œ ì˜¤ë””ì˜¤ ì„ë² ë”©ì„ ë³€ì¡°í•œ í›„ cross attention

```python
class ResidualAttentionBlock_Gate(nn.Module):
    def __init__(self, d_model: int, n_head: int):
        super().__init__()
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        # Text-to-audio adapter
        self.text_to_audio_adapter = nn.Sequential(OrderedDict([
            ("adapter_fc", nn.Linear(d_model, d_model)),
            ("adapter_gelu", QuickGELU()),
            ("adapter_proj", nn.Linear(d_model, d_model))
        ]))
        
    def forward(self, para_tuple: tuple):
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        # Transform audio based on text
        text_guidance = self.text_to_audio_adapter(t_mean).unsqueeze(0)
        conditioned_audio = v + text_guidance  # or v * sigmoid(text_guidance)
        
        # Use conditioned audio for cross attention
        x = x + self.cross_attention(self.ln_3(fused_query), conditioned_audio, attn_mask/100) * attn_gate
```

**ê¸°ëŒ€ íš¨ê³¼**:
- í…ìŠ¤íŠ¸ê°€ "ì–´ë–¤ ì˜¤ë””ì˜¤ ì •ë³´ë¥¼ ì£¼ëª©í• ì§€" ì§ì ‘ ì œì–´
- ì˜ˆ: "dog barking" â†’ ê°œ ì§–ëŠ” ì†Œë¦¬ì— í•´ë‹¹í•˜ëŠ” ì£¼íŒŒìˆ˜ ì˜ì—­ ê°•ì¡°

**ë‚œì´ë„**: â­â­â­â˜†â˜† (ì¤‘ê°„)  
**íš¨ê³¼**: â­â­â­â­â˜† (ë†’ìŒ)

---

#### 6ï¸âƒ£ **Dual-Path Fusion (ë³‘ë ¬ ê²½ë¡œ)** â­â­â­â­â­

**í•µì‹¬ ì•„ì´ë””ì–´**: Videoâ†’Audioì™€ Textâ†’Audioë¥¼ ê°ê° ê³„ì‚° í›„ ìœµí•©

```python
def forward(self, para_tuple: tuple):
    # ... ê¸°ì¡´ ì½”ë“œ ...
    
    # Path 1: Video queries audio
    video_audio_fusion = self.cross_attention(self.ln_3(x), v, attn_mask/100)
    
    # Path 2: Text queries audio
    text_audio_fusion = self.cross_attention(self.ln_3(t), v, attn_mask/100)
    
    # Dynamic path weighting
    path_weight = self.query_gate(torch.cat((x_mean, v_mean, t_mean), dim=1)).sigmoid()
    
    # Combine paths
    final_fusion = video_audio_fusion * path_weight + text_audio_fusion * (1 - path_weight)
    
    x = x + final_fusion * attn_gate
```

**ê¸°ëŒ€ íš¨ê³¼**:
- Videoì™€ Textê°€ ì˜¤ë””ì˜¤ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ì •ë³´ë¥¼ ì¶”ì¶œ
- ë” í’ë¶€í•œ multimodal representation

**ë‚œì´ë„**: â­â­â­â­â˜† (ì–´ë ¤ì›€)  
**íš¨ê³¼**: â­â­â­â­â­ (ë§¤ìš° ë†’ìŒ)

---

### ğŸ¥‰ Priority 3: ê³ ê¸‰ ê°œì„  (ì—°êµ¬ ìˆ˜ì¤€)

#### 7ï¸âƒ£ **Dynamic Layer Allocation** â­â­â­â­â­

**í•µì‹¬ ì•„ì´ë””ì–´**: ì…ë ¥ì— ë”°ë¼ ì–´ë–¤ ë ˆì´ì–´ë¥¼ í™œì„±í™”í• ì§€ ë™ì ìœ¼ë¡œ ê²°ì •

```python
class Transformer_Gate(nn.Module):
    def __init__(self, width: int, layers: int, heads: int):
        super().__init__()
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        # Layer router
        self.layer_router = nn.Sequential(
            nn.Linear(width * 3, layers),
            nn.Softmax(dim=-1)
        )
        
    def forward(self, q, v, t, attn_mask=None):
        # Compute layer importance
        combined = torch.cat([q.mean(0), v.mean(0), t.mean(0)], dim=-1)
        layer_weights = self.layer_router(combined)  # [batch, num_layers]
        
        # Weighted sum of layer outputs
        x = q
        for i, block in enumerate(self.resblocks):
            x_new = block((x, v, t, attn_mask, [], [], []))[0]
            weight = layer_weights[:, i].view(1, -1, 1)
            x = x + (x_new - x) * weight  # Interpolate
```

**ê¸°ëŒ€ íš¨ê³¼**:
- ì‰¬ìš´ ìƒ˜í”Œ: ì ì€ ë ˆì´ì–´ë§Œ ì‚¬ìš© (íš¨ìœ¨ì„± â†‘)
- ì–´ë ¤ìš´ ìƒ˜í”Œ: ëª¨ë“  ë ˆì´ì–´ ì‚¬ìš© (í‘œí˜„ë ¥ â†‘)

**ë‚œì´ë„**: â­â­â­â­â­ (ë§¤ìš° ì–´ë ¤ì›€)  
**íš¨ê³¼**: â­â­â­â­â­ (ë§¤ìš° ë†’ìŒ)

---

#### 8ï¸âƒ£ **Contrastive Gate Learning** â­â­â­â­

**í•µì‹¬ ì•„ì´ë””ì–´**: Gate ê°’ì„ contrastive learningìœ¼ë¡œ í•™ìŠµ

```python
# Training ì‹œ ì¶”ê°€ loss
def compute_gate_contrastive_loss(query_gate_list, attn_gate_list, labels):
    """
    ê¸ì • ìŒ(positive pairs)ì€ ë¹„ìŠ·í•œ gate íŒ¨í„´ì„ ê°€ì ¸ì•¼ í•¨
    ë¶€ì • ìŒ(negative pairs)ì€ ë‹¤ë¥¸ gate íŒ¨í„´ì„ ê°€ì ¸ì•¼ í•¨
    """
    # Stack all gates: [batch, num_layers, 1]
    query_gates = torch.stack(query_gate_list, dim=1)
    attn_gates = torch.stack(attn_gate_list, dim=1)
    
    # Compute similarity
    gate_similarity = F.cosine_similarity(
        query_gates.view(batch, -1).unsqueeze(1),
        query_gates.view(batch, -1).unsqueeze(0),
        dim=-1
    )
    
    # Contrastive loss
    positive_mask = (labels.unsqueeze(1) == labels.unsqueeze(0)).float()
    loss = F.mse_loss(gate_similarity, positive_mask)
    
    return loss
```

**ê¸°ëŒ€ íš¨ê³¼**:
- ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¹„ë””ì˜¤ëŠ” ë¹„ìŠ·í•œ gating íŒ¨í„´ í•™ìŠµ
- Interpretability í–¥ìƒ

**ë‚œì´ë„**: â­â­â­â­â˜† (ì–´ë ¤ì›€)  
**íš¨ê³¼**: â­â­â­â­â˜† (ë†’ìŒ)

---

#### 9ï¸âƒ£ **Adaptive Query Pooling** â­â­â­â­

**í•µì‹¬ ì•„ì´ë””ì–´**: Mean pooling ëŒ€ì‹  attention-based pooling ì‚¬ìš©

```python
class ResidualAttentionBlock_Gate(nn.Module):
    def __init__(self, d_model: int, n_head: int):
        super().__init__()
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        # Attention pooling
        self.pool_query = nn.Linear(d_model, 1)
        
    def adaptive_pool(self, x):
        # x: [seq_len, batch, d_model]
        weights = F.softmax(self.pool_query(x), dim=0)  # [seq_len, batch, 1]
        pooled = (x * weights).sum(dim=0)  # [batch, d_model]
        return pooled
        
    def forward(self, para_tuple: tuple):
        x, v, t, attn_mask, attn_gate_list, ff_gate_list, query_gate_list = para_tuple
        
        # Adaptive pooling instead of mean
        x_pooled = self.adaptive_pool(x)
        v_pooled = self.adaptive_pool(v)
        t_pooled = self.adaptive_pool(t)
```

**ê¸°ëŒ€ íš¨ê³¼**:
- ì¤‘ìš”í•œ í”„ë ˆì„/í† í°ì— ë” ì§‘ì¤‘
- Mean poolingë³´ë‹¤ ì •ë³´ ë³´ì¡´

**ë‚œì´ë„**: â­â­â­â˜†â˜† (ì¤‘ê°„)  
**íš¨ê³¼**: â­â­â­â­â˜† (ë†’ìŒ)

---

## ğŸ¯ ì¶”ì²œ êµ¬í˜„ ìˆœì„œ

ì‹¤í—˜ íš¨ìœ¨ì„±ê³¼ íš¨ê³¼ë¥¼ ê³ ë ¤í•œ ì¶”ì²œ ìˆœì„œ:

### Phase 1: Quick Wins (1-2ì£¼)
1. âœ… **Layer-Aware Gating** (ë°©ë²• A: Simple Decay)
   - êµ¬í˜„ ê°„ë‹¨, íš¨ê³¼ ê²€ì¦ë¨
   
2. âœ… **Residual Gate**
   - í•œ ì¤„ ìˆ˜ì •ìœ¼ë¡œ ê°€ëŠ¥
   
3. âœ… **Attention Temperature Scaling**
   - Parameter í•˜ë‚˜ ì¶”ê°€

### Phase 2: Performance Boost (2-3ì£¼)
4. âœ… **Multi-Head Query Fusion**
   - ì„±ëŠ¥ í–¥ìƒ ê¸°ëŒ€ì¹˜ ë†’ìŒ
   
5. âœ… **Text-Conditioned Audio Transformation**
   - ë…¼ë¦¬ì ìœ¼ë¡œ íƒ€ë‹¹í•¨

### Phase 3: Advanced (1ê°œì›”+)
6. âœ… **Dual-Path Fusion**
   - ì•„í‚¤í…ì²˜ ë³€ê²½ í•„ìš”
   
7. âœ… **Adaptive Query Pooling**
   - Mean â†’ Attention pooling

### Phase 4: Research Level (ì„ íƒì )
8. Dynamic Layer Allocation
9. Contrastive Gate Learning

---

## ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì • ì§€í‘œ

ê° ê°œì„ ì‚¬í•­ì˜ íš¨ê³¼ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•œ ì§€í‘œ:

### 1. Retrieval Metrics
- R@1, R@5, R@10 (Textâ†’Video, Videoâ†’Text)
- Median Rank
- Mean Rank

### 2. Gate Analysis
```python
# ì €ì¥í•  í†µê³„ëŸ‰
gate_stats = {
    'query_gate': {
        'mean_per_layer': [...],
        'std_per_layer': [...],
        'distribution': [...]  # Histogram
    },
    'attn_gate': {...},
    'ff_gate': {...}
}
```

### 3. Ablation Studies
- w/o Query Fusion
- w/o Layer-Aware Gating
- w/o Text Conditioning
- ë“±ë“±...

---

## ğŸ”§ ë””ë²„ê¹… íŒ

### Gate ê°’ ì‹œê°í™”
```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_gates(query_gates, attn_gates, ff_gates, save_path):
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    # Query gate distribution
    sns.violinplot(data=query_gates, ax=axes[0])
    axes[0].set_title('Query Gate Distribution per Layer')
    
    # Attention gate
    sns.violinplot(data=attn_gates, ax=axes[1])
    axes[1].set_title('Attention Gate Distribution per Layer')
    
    # FF gate
    sns.violinplot(data=ff_gates, ax=axes[2])
    axes[2].set_title('FF Gate Distribution per Layer')
    
    plt.tight_layout()
    plt.savefig(save_path)
```

### Gate ê°’ ëª¨ë‹ˆí„°ë§
```python
# Training loopì— ì¶”ê°€
if step % 100 == 0:
    # Extract gate values from last batch
    query_gate_values = [g.mean().item() for g in query_gate_list]
    print(f"Layer-wise Query Gates: {query_gate_values}")
    
    # Check for anomalies
    if any(g < 0.01 or g > 0.99 for g in query_gate_values):
        print("WARNING: Gate saturation detected!")
```

---

## ğŸ’¡ ì¶”ê°€ ì•„ì´ë””ì–´ (ë¸Œë ˆì¸ìŠ¤í† ë°)

### 1. Hierarchical Gating
- Coarse-grained gate (ì „ì²´ ëª¨ë‹¬ë¦¬í‹° ì„ íƒ)
- Fine-grained gate (feature-level ì„ íƒ)

### 2. Uncertainty-Aware Gating
- Gate ê°’ê³¼ í•¨ê»˜ confidenceë„ ì¶œë ¥
- ë¶ˆí™•ì‹¤í•  ë•ŒëŠ” ensemble íš¨ê³¼

### 3. Cross-Modal Attention Reweighting
- Self-attention í›„ cross-attentionì˜ ê°€ì¤‘ì¹˜ ì¬ì¡°ì •
- More context-aware fusion

### 4. Temporal Gate Smoothing
- ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ì—ì„œ ì‹œê°„ì ìœ¼ë¡œ ë¶€ë“œëŸ¬ìš´ gate ë³€í™”
- Temporal consistency loss

---

## ğŸ“š ì°¸ê³ í•  ë§Œí•œ ë…¼ë¬¸ë“¤

1. **Layer-wise Adaptation**
   - "AdaViT: Adaptive Tokens for Efficient Vision Transformer" (CVPR 2022)
   - "Dynamic DETR: End-to-End Object Detection with Dynamic Attention" (ICCV 2021)

2. **Multi-Modal Fusion**
   - "MDETR: Modulated Detection for End-to-End Multi-Modal Understanding" (ICCV 2021)
   - "CLIP-ViL: How Much Can CLIP Benefit Vision-and-Language Tasks?" (ICLR 2022)

3. **Gated Mechanisms**
   - "Gated Fusion Network for Single Image Dehazing" (CVPR 2018)
   - "Dynamic Fusion with Intra- and Inter-modality Attention Flow" (CVPR 2019)

---

## âœï¸ ìµœì¢… ì¶”ì²œ

**1ìˆœìœ„ë¡œ êµ¬í˜„í•  ê²ƒ**: Layer-Aware Gating (Simple Decay)
- ì´ìœ : êµ¬í˜„ ë§¤ìš° ê°„ë‹¨ (10ì¤„), íš¨ê³¼ í™•ì‹¤, ë…¼ë¬¸ì—ì„œë„ ìì£¼ ì‚¬ìš©

**2ìˆœìœ„**: Multi-Head Query Fusion
- ì´ìœ : í‘œí˜„ë ¥ ëŒ€í­ í–¥ìƒ, ë‹¤ì–‘í•œ fusion ì „ëµ í•™ìŠµ ê°€ëŠ¥

**3ìˆœìœ„**: Text-Conditioned Audio Transformation
- ì´ìœ : ì§ê´€ì ì´ê³  í•´ì„ ê°€ëŠ¥, ì¶”ê°€ íŒŒë¼ë¯¸í„° ì ë‹¹

ì´ ìˆœì„œëŒ€ë¡œ í•˜ë‚˜ì”© êµ¬í˜„í•˜ê³  ì‹¤í—˜í•˜ë©´ì„œ ì„±ëŠ¥ ë³€í™”ë¥¼ ê´€ì°°í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤! ğŸš€
